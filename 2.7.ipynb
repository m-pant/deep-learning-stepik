{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cbb5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.950\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# здесь объявляйте класс TriagModel\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "total = 100\n",
    "x_train = torch.randint(1, 10, (total, 3), dtype=torch.float32)\n",
    "y_train = x_train.sum(dim=1) / 3\n",
    "\n",
    "# здесь создавайте модель (model)\n",
    "class TriagModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(3, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "# # переведите модель в режим обучения\n",
    "model = TriagModel()\n",
    "model.train()\n",
    "lr = 0.01 # шаг обучения\n",
    "N = 1000 # число итераций SGD\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)# здесь сформируйте оптимизатор Adam с параметрами модели и шагом обучения lr\n",
    "loss_func = nn.MSELoss() # здесь создайте функцию потерь с помощью класса nn.MSELoss\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, total)\n",
    "    # пропустите через модель k-й образ выборки x_train и вычислите прогноз predict\n",
    "    y_pred = model(x_train[k])\n",
    "    loss = loss_func(y_pred,y_train[k].unsqueeze(0)) # вычислите значение функции потерь и сохраните результат в переменной loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Testing\n",
    "dec = -1 # number of dec for round\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for x, y in zip(x_train , y_train):\n",
    "        y_pred = model(x)\n",
    "        preds.append(y_pred)\n",
    "\n",
    "y_preds = torch.tensor(preds)\n",
    "mask = (y_preds.round(decimals=dec) == y_train.round(decimals=dec)).float().mean()\n",
    "# accuracy = mask.sum() / mask.shape[0]\n",
    "print(f\"Accuracy: {mask.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899999856948853\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# здесь объявляйте класс ClassModel\n",
    "class ClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2,3)\n",
    "        self.layer2 = nn.Linear(3,1)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# обучающая выборка: x_train - входные значения; y_train - целевые значения\n",
    "x_train = torch.tensor([(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)])\n",
    "y_train = torch.FloatTensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "\n",
    "model = ClassModel() # здесь создавайте модель\n",
    "# print(model)\n",
    "# print(list(model.parameters()))\n",
    "# переведите модель в режим обучения\n",
    "model.train()\n",
    "\n",
    "total = x_train.size(0) # размер обучающей выборки\n",
    "N = 1000 # число итераций алгоритма SGD\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01) # задайте оптимизатор Adam с шагом обучения lr=0.01\n",
    "loss_func = nn.BCEWithLogitsLoss() # сформируйте функцию потерь (бинарную кросс-энтропию) с помощью класса nn.BCEWithLogitsLoss\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, total)\n",
    "    # пропустите через модель k-й образ выборки x_train и вычислите прогноз predict\n",
    "    y_pred = model(x_train[k])\n",
    "    y_train_k =  y_train[k].unsqueeze(0)\n",
    "    loss = loss_func(y_pred, y_train_k) # вычислите значение функции потерь и сохраните результат в переменной loss\n",
    "    \n",
    "    # выполните один шаг градиентного спуска так, как это было сделано в предыдущем подвиге\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# переведите модель в режим эксплуатации\n",
    "model.eval()\n",
    "# прогоните через модель обучающую выборку и подсчитайте долю верных классификаций\n",
    "\n",
    "y_preds = []\n",
    "with torch.no_grad():\n",
    "    for x in x_train:\n",
    "        y_pr = model(x)\n",
    "        y_preds.append( (y_pr.sign()+1)/2 )\n",
    "\n",
    "# результат (долю верных классификаций) сохраните в переменной Q (в виде вещественного числа, а не тензора)\n",
    "Q = (torch.tensor(y_preds) == y_train ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14dcadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "# 7 \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# сюда скопируйте класс ClassModel из предыдущего подвига\n",
    "class ClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2,3)\n",
    "        self.layer2 = nn.Linear(3,1)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# обучающая выборка: x_train - входные значения; y_train - целевые значения\n",
    "x_train = torch.tensor([(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)])\n",
    "y_train = torch.FloatTensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "\n",
    "model = ClassModel() # здесь создавайте модель\n",
    "# переведите модель в режим обучения\n",
    "\n",
    "total = x_train.size(0) # размер обучающей выборки\n",
    "N = 1000 # число итераций алгоритма SGD\n",
    "batch_size = 8 # размер мини-батча\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01) # оптимизатор Adam с шагом обучения lr=0.01\n",
    "loss_func = nn.BCEWithLogitsLoss() # функция потерь: бинарная кросс-энтропия, класс nn.BCEWithLogitsLoss\n",
    "\n",
    "for _ in range(N):\n",
    "    idx = np.random.choice(total, batch_size, False) # выбор индексов образов в размере batch_size\n",
    "    # с помощью списочной индексации отберите из выборки x_train образы согласно индексам списка idx\n",
    "    y_pred = model(x_train[idx])\n",
    "    y_train_k  = y_train[idx].unsqueeze(1)\n",
    "    loss = loss_func(y_pred, y_train_k) # вычислите значение функции потерь и сохраните результат в переменной loss\n",
    "\n",
    "    # выполните один шаг градиентного спуска так, как это было сделано в предыдущем подвиге\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# переведите модель в режим эксплуатации\n",
    "model.eval()\n",
    "# прогоните через модель обучающую выборку и подсчитайте долю верных классификаций\n",
    "\n",
    "y_preds = []\n",
    "with torch.no_grad():\n",
    "    for x in x_train:\n",
    "        y_pr = model(x)\n",
    "        y_preds.append( (y_pr.sign()+1)/2 )\n",
    "\n",
    "# результат (долю верных классификаций) сохраните в переменной Q (в виде вещественного числа, а не тензора)\n",
    "Q = (torch.tensor(y_preds) == y_train ).float().mean()\n",
    "print(Q.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935f09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
